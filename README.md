This project is “Creating ETL Data Pipelines with BashOperator using Apache Airflow” and “Creating Streaming Data Pipelines using Kafka”. 
To achieve the result. A DAG was created which comprised of the following processes:

•	Extract data from CSV, TSV, and fixed width files
•	Transform extracted data
•	Load transformed data into the staging area.
•	Submit, unpause, and monitor a DAG.
•	Create a topic in Kafka
•	Download and customize a streaming data consumer
•	Verify that streaming data has been collected in a database table
